<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Feature Selection via Gain Penalization in Random Forests</title>
  
  <meta property="description" itemprop="description" content="A method for feature selection with Random Forests"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2021-03-25"/>
  <meta property="article:created" itemprop="dateCreated" content="2021-03-25"/>
  <meta name="article:author" content="Bruna Wundervald"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Feature Selection via Gain Penalization in Random Forests"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="A method for feature selection with Random Forests"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Feature Selection via Gain Penalization in Random Forests"/>
  <meta property="twitter:description" content="A method for feature selection with Random Forests"/>
  
  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="Feature Selection via Gain Penalization in Random Forests"/>
  <meta name="citation_fulltext_html_url" content="https://ieeexplore.ieee.org/document/9229097"/>
  <meta name="citation_doi" content="10.1109/ACCESS.2020.3032095"/>
  <meta name="citation_journal_title" content="IEEE Access"/>
  <meta name="citation_online_date" content="2021/03/25"/>
  <meta name="citation_publication_date" content="2021/03/25"/>
  <meta name="citation_author" content="Bruna Wundervald"/>
  <meta name="citation_author_institution" content="Hamilton Institute, Maynooth University"/>
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Bagging predictors;citation_publication_date=1996;citation_volume=24;citation_doi=10.1023/A:1018054314350;citation_issn=1573-0565;citation_author=Leo Breiman"/>
  <meta name="citation_reference" content="citation_title=Double regularization methods for robust feature selection and svm classification via dc programming;citation_publication_date=2018;citation_publisher=Elsevier;citation_volume=429;citation_author=Julio López;citation_author=Sebastián Maldonado;citation_author=Miguel Carrasco"/>
  <meta name="citation_reference" content="citation_title=A prognostic dna signature for t1t2 node-negative breast cancer patients;citation_publication_date=2010;citation_publisher=Wiley Online Library;citation_volume=49;citation_author=Eléonore Gravier;citation_author=Gaëlle Pierron;citation_author=Anne Vincent-Salomon;citation_author=Nadège Gruel;citation_author=Virginie Raynal;citation_author=Alexia Savignoni;citation_author=Yann De Rycke;citation_author=Jean-Yves Pierga;citation_author=Carlo Lucchesi;citation_author=Fabien Reyal;citation_author=A prognostic dna signature for t1t2 node-negative breast cancer patients"/>
  <meta name="citation_reference" content="citation_title=Independently interpretable lasso: A new regularizer for sparse regression with uncorrelated variables;citation_publication_date=2018;citation_publisher=PMLR;citation_author=Masaaki Takada;citation_author=Taiji Suzuki;citation_author=Hironori Fujisawa"/>
  <meta name="citation_reference" content="citation_title=Improvements in the large p, small n classification issue;citation_publication_date=2020;citation_publisher=Springer;citation_volume=1;citation_author=Phuoc-Hai Huynh;citation_author=Thanh-Nghi Do;citation_author=Improvements in the large p, small n classification issue"/>
  <meta name="citation_reference" content="citation_title=Understanding random forests: From theory to practice;citation_publication_date=2014;citation_doi=10.13140/2.1.1570.5928;citation_author=Gilles Louppe"/>
  <meta name="citation_reference" content="citation_title=Generalizing gain penalization for feature selection in tree-based models;citation_publication_date=2020;citation_publisher=IEEE;citation_volume=8;citation_author=Bruna Wundervald;citation_author=Andrew C Parnell;citation_author=Katarina Domijan"/>
  <meta name="citation_reference" content="citation_title=Feature selection via regularized trees;citation_publication_date=2012;citation_publisher=IEEE;citation_author=Houtao Deng;citation_author=George Runger"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","bibliography","citation_url","journal","doi","output"]}},"value":[{"type":"character","attributes":{},"value":["Feature Selection via Gain Penalization in Random Forests"]},{"type":"character","attributes":{},"value":["A method for feature selection with Random Forests\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Bruna Wundervald"]},{"type":"character","attributes":{},"value":["http://brunaw.com/"]},{"type":"character","attributes":{},"value":["Hamilton Institute, Maynooth University"]}]}]},{"type":"character","attributes":{},"value":["03-25-2021"]},{"type":"character","attributes":{},"value":["references.bib"]},{"type":"character","attributes":{},"value":["https://ieeexplore.ieee.org/document/9229097"]},{"type":"character","attributes":{},"value":["IEEE Access"]},{"type":"character","attributes":{},"value":["10.1109/ACCESS.2020.3032095"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["post_files/bowser-1.9.3/bowser.min.js","post_files/distill-2.2.21/template.v2.js","post_files/figure-html5/unnamed-chunk-12-1.png","post_files/figure-html5/unnamed-chunk-13-1.png","post_files/figure-html5/unnamed-chunk-21-1.png","post_files/jquery-1.11.3/jquery.min.js","post_files/kePrint-0.0.1/kePrint.js","post_files/lightable-0.0.1/lightable.css","post_files/webcomponents-2.0.0/webcomponents.js","references.bib","results/final_results.rds","results/final_vars.rds","results/folds_imp_head.rds","results/folds_imp.rds","results/metric_std_rf.rds","results/results_reev.rds","results/results_table.rds","results/results.rds","results/run_all_models.rds","results/Untitled.R"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="post_files/kePrint-0.0.1/kePrint.js"></script>
  <link href="post_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
  <script src="post_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="post_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="post_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="post_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Feature Selection via Gain Penalization in Random Forests","description":"A method for feature selection with Random Forests","doi":"10.1109/ACCESS.2020.3032095","authors":[{"author":"Bruna Wundervald","authorURL":"http://brunaw.com/","affiliation":"Hamilton Institute, Maynooth University","affiliationURL":"#"}],"publishedDate":"2021-03-25T00:00:00.000+00:00","citationText":"Wundervald, 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Feature Selection via Gain Penalization in Random Forests</h1>
<p><p>A method for feature selection with Random Forests</p></p>
</div>

<div class="d-byline">
  Bruna Wundervald <a href="http://brunaw.com/" class="uri">http://brunaw.com/</a> (Hamilton Institute, Maynooth University)
  
<br/>03-25-2021
</div>

<div class="d-article">
<h3 class="d-toc-header">Table of Contents</h3>
<nav class="d-toc" id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#what-is-gain-penalization">What is Gain Penalization?</a></li>
<li><a href="#the-full-feature-selection-procedure">The full feature selection procedure</a><ul>
<li><a href="#things-to-have-in-mind-when-running-the-penalized-rf">Things to have in mind when running the penalized RF</a></li>
</ul></li>
<li><a href="#implementation">Implementation</a><ul>
<li><a href="#how-does-this-compare-to-the-literature">How does this compare to the literature?</a></li>
</ul></li>
</ul>
</nav>
<hr class="d-toc-separator"/>
<h2 id="introduction">Introduction</h2>
<p>Decision trees ensembles are a very popular type of machine learning algorithm, which is mostly due to their adaptive nature, high prediction power and, in some sense, interpretability. Random Forests are one form of such ensembles, and they consist of growing many trees in re-samples of the data, and averaging their results at end, creating a bagged ensemble described <span class="citation" data-cites="Breiman1996">(Breiman <a href="#ref-Breiman1996" role="doc-biblioref">1996</a>)</span> by</p>
<p><span class="math display">\[\begin{equation} 
\hat f(\mathbf{x}) = \sum_{n = 1}^{N_{tree}} \frac{1}{N_{tree}} \hat f_n(\mathbf{x}),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat f_n\)</span> corresponds to the <span class="math inline">\(n\)</span>-th tree. However, even though we can name many very good qualities of the Random Forests, we also know that they don’t do feature selection very well. However, Random Forests usually use all or most of the features that are feed to them, and they struggle a lot to detect highly correlated features <span class="citation" data-cites="phdthesisRF">(Louppe <a href="#ref-phdthesisRF" role="doc-biblioref">2014</a>)</span>, that ideally shouldn’t be used in an algorithm more than once. In a situation where predictions are hard or expensive to obtain (e.g. genetic related data such as SNPs, peptides or proteins), this becomes a relevant issue that needs to be addressed if we realistically want to use RFs for such prediction tasks.</p>
<p>In this post, I will give a general overview of feature selection in Random Forests using gain penalization. The <code>R</code> code is provided along with the explanation, and I’ll often be referring to my own paper on the subject <span class="citation" data-cites="wundervald2020generalizing">(Wundervald, Parnell, and Domijan <a href="#ref-wundervald2020generalizing" role="doc-biblioref">2020</a>)</span>. A few auxiliary functions are used throughout the code, and they can be found <a href="https://github.com/brunaw/reg-rf-demo/blob/master/code">here</a>.</p>
<h2 id="what-is-gain-penalization">What is Gain Penalization?</h2>
<p>The idea of doing feature selection via gain penalization was first introduced in <span class="citation" data-cites="rrf_paper">(Deng and Runger <a href="#ref-rrf_paper" role="doc-biblioref">2012</a>)</span>, and it is basically a gain weighting method, done during the greedy procedure step of a tree estimation. In other words, when determining the next child node to be added to a decision tree, the gain (or the error reduction) of each feature is multiplied by a penalization parameter. With this, a new split will only be made if, after the penalization, the gain of adding this node is still higher than having no new child node in the tree. This new penalized gain is written as</p>
<p><span class="math display">\[\begin{equation}
\text{Gain}_{R}(\mathbf{X}_{i}, t) = 
\begin{cases}
\lambda \Delta(i, t), \thinspace  i \notin \mathbb{U} \text{ and} \\
\Delta(i, t), \thinspace  i \in \mathbb{U}, 
\end{cases}
\label{eq:grrf}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbb{U}\)</span> is the set of indices of the features previously used in the tree, <span class="math inline">\(\mathbf{X}_{i}\)</span> is the candidate feature, <span class="math inline">\(t\)</span> is the candidate splitting point and <span class="math inline">\(\lambda \in (0, 1]\)</span>.</p>
<p>In our paper <span class="citation" data-cites="wundervald2020generalizing">(Wundervald, Parnell, and Domijan <a href="#ref-wundervald2020generalizing" role="doc-biblioref">2020</a>)</span>, we proposed a generalization to the way the penalization coefficients are calculated, such that we can have full control over it. Our <span class="math inline">\(\lambda_i\)</span> is written as</p>
<p><span class="math display">\[\begin{equation}
\lambda_i = (1 - \gamma) \lambda_0 + \gamma g(\mathbf{x}_i),
\label{eq:generalization}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\lambda_0 \in [0, 1)\)</span> is interpreted as the baseline regularization, <span class="math inline">\(g(\mathbf{x}_i)\)</span> is a function of the <span class="math inline">\(i\)</span>-th feature, and <span class="math inline">\(\gamma \in [0, 1)\)</span> is their mixture parameter, with <span class="math inline">\(\lambda_i \in [0, 1)\)</span>. The idea behind this composition is creating a local-global form of penalization, since the equation mixes how much all features are jointly (globally) penalized and how much it is due to a local <span class="math inline">\(g(\mathbf{x}_i)\)</span>, which is manually defined. This <span class="math inline">\(g(\mathbf{x}_i)\)</span>, by its turn, should represent relevant information about the features, based on some characteristic of interest (correlation to the target, for example). This formulation also has inspiration on the use of priors made in Bayesian methods, since we introduce “prior knowledge” regarding the importance of each feature into the model (likewise, the data will tell us how strong our assumptions about the penalization are, since even if we try to penalize a truly important feature, its gain will be high enough to overcome the penalization and the feature will get selected by the algorithm).</p>
<p>In this blog post, I’ll use two different types of <span class="math inline">\(g(\mathbf{x}_i)\)</span>:</p>
<ol type="1">
<li><p>The Mutual Information between each feature and the target variable <span class="math inline">\(y\)</span> (normalized to be between 0 and 1)</p></li>
<li><p>The variable importance values obtained from a previously run standard Random Forest, which is what I call a <em>Boosted</em> <span class="math inline">\(g(\mathbf{x}_i)\)</span><br />
(also normalized to be between 0 and 1)</p></li>
</ol>
<p>For more details on those functions and other options, please see the paper <span class="citation" data-cites="wundervald2020generalizing">(Wundervald, Parnell, and Domijan <a href="#ref-wundervald2020generalizing" role="doc-biblioref">2020</a>)</span>.</p>
<h2 id="the-full-feature-selection-procedure">The full feature selection procedure</h2>
<p>In general, the penalized random forest model is not the one that will be used for the final predictions. Instead, I prefer to use the method described before as a tool to first select the best features possible, and then have a final random forest that uses such features. This full feature selection procedure happens in 3 main steps:</p>
<ol type="1">
<li>We run a bunch of penalized random forests models with different hyperparameters and record their accuracies and final set of features</li>
<li>For each training dataset, select the top-n (for this post we use n = 3) fitted models in terms of the accuracies, and run a “new” random forest for each of the feature sets used by them. This is done using all of the training sets so we can evaluate how these features perform in slightly different scenarios</li>
<li>Finally, get the top-m set of models (here m = 30) from these new ones, check which features were the most used between them and run a final random forest model with this feature set. In this post I select only the 15 most used features from the top 30 models, but both numbers can be changed depending on the situation</li>
</ol>
<p>All this is to make sure that the features used in the final model are, indeed, very good. This might sound a bit exhaustive but to me it pays off knowing that out of a few thousand variables, I’ll manage to select only a few and still have a powerful and generalizable model.</p>
<h3 id="things-to-have-in-mind-when-running-the-penalized-rf">Things to have in mind when running the penalized RF</h3>
<ul>
<li><p>You can add an “extra penalization” when the new variable is to be picked at a deep node in a tree (for details please see the paper)</p></li>
<li><p>The <code>mtry</code> hyperparameter requires attention (and even proper tuning), since it is known that to affect the prediction power of random forests and, is our case, the penalized random forests</p></li>
<li><p>Ideally, the <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\lambda_0\)</span> and <code>mtry</code> hyperparameters should be tuned, or set based on the experience of the person running the algorithms, but for the time being we’ll be using a few predefined values (kind of like grid search)</p></li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>Let us consider the <code>gravier</code> dataset <span class="citation" data-cites="gravier2010prognostic">(Gravier et al. <a href="#ref-gravier2010prognostic" role="doc-biblioref">2010</a>)</span>, for which the goal is to predict whether 168 breast cancer patients had a diagnosis labelled “poor” (~66%) or “good” (~33%), based on a a set of 2905 predictors. In this first part of the code, we’ll just load the data and create our 5-fold cross validation object, which will be used to create 5 different train and test sets. As of usual, there will be lots of <code>tidyverse</code> and <code>tidymodels</code> functions throughout my code:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tidyverse)
library(tidymodels)
library(infotheo) # For the mutual information function
set.seed(2021)

# Loading data and creating a 5-fold CV object
data(&#39;gravier&#39;, package = &#39;datamicroarray&#39;)

gravier &lt;- data.frame(class = gravier$y, gravier$x)
folds &lt;- rsample::vfold_cv(gravier, v = 5) %&gt;% 
  dplyr::mutate(train =  map(splits, training),
                test  = map(splits, testing))</code></pre>
</div>
<p>With this done, we can start the actual modelling steps of the code. I will be using a few of auxiliary functions, which are given <a href="https://github.com/brunaw/reg-rf-demo/blob/master/code"><strong>here</strong></a>, but the two following functions are explicitly shown in this post because they’re very important. The <code>modelling()</code> function will be used to run the random forests algorithms, and it’s written in a way that I can change the <code>mtry</code> hyperparameter, the penalization coefficients. At this point we’ll be feeding all the 2905 features, and letting the gain penalization perform the feature selection for us. The second function shown below is <code>penalization()</code>, which implements the calculation of two different types of penalization: one that takes <span class="math inline">\(g(\mathbf{x}_i)\)</span> to be the normalized mutual information between the target and each feature, and one that I call a “Boosted” <span class="math inline">\(g(\mathbf{x}_i)\)</span>, because it depends on the normalized importance values of a previously calculated random forest (for more details, see <span class="citation" data-cites="wundervald2020generalizing">Wundervald, Parnell, and Domijan (<a href="#ref-wundervald2020generalizing" role="doc-biblioref">2020</a>)</span>).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# A function that run the penalized random forests models 
modelling &lt;- function(train, reg_factor = 1, mtry = 1){
  rf_mod &lt;- 
    rand_forest(trees = 500, mtry = (mtry * ncol(train)) - 1) %&gt;% 
    set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;, 
               regularization.factor = reg_factor) %&gt;% 
    set_mode(&quot;classification&quot;) %&gt;% 
    parsnip::fit(class ~ ., data = train)
  return(rf_mod)
}
# A function that receives the mixing parameters
# and calculates lambda_i with the chose g(x_i)
penalization &lt;- function(gamma, lambda_0, data = NULL, imps = NULL, type = &quot;rf&quot;){
  if(type == &quot;rf&quot;){
    # Calculating the normalized importance values 
    imps &lt;- imps/max(imps)
    imp_mixing &lt;- (1 - gamma) * lambda_0 + imps * gamma 
    return(imp_mixing)
  } else if(type == &quot;MI&quot;){
    mi &lt;- function(data, var) mutinformation(c(data$class), data %&gt;% pull(var))
    
    # Calculating the normalized mutual information values
    disc_data  &lt;- infotheo::discretize(data) 
    disc_data$class &lt;- as.factor(data$class)
    names_data &lt;- names(data)[-1]
    mi_vars &lt;- names_data  %&gt;% map_dbl(~{mi(data = disc_data, var = .x) })
    mi_mixing &lt;- (1 - gamma) * lambda_0 + gamma * (mi_vars/max(mi_vars))
    return(mi_mixing)  
  }
}</code></pre>
</div>
<p>The code below creates the combinations of all hypeparameter values used here, for the <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\lambda_0\)</span> and <code>mtry</code> hyperparameters. After that, we calculate the two <span class="math inline">\(g(\mathbf{x}_i)\)</span> for each training set, and their final coefficient penalization values by combining them with <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda_0\)</span> to create the penalization mixture (as described previously), for each of the 5 training sets.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# Setting all parameters ---
mtry &lt;-  tibble(mtry = c(0.20, 0.45, 0.85))  
gamma_f  &lt;-  c(0.3, 0.5, 0.8)
lambda_0_f &lt;- c(0.35, 0.75)

parameters &lt;- mtry %&gt;% tidyr::crossing(lambda_0_f, gamma_f)
# Adds gamma_f and lambda_0_f and run the functions with them ------
folds_imp &lt;- folds %&gt;% 
  dplyr::mutate(
    # Run the standard random forest model for the 5 folds
    model = purrr::map(train, modelling), 
    importances_std = purrr::map(model, ~{.x$fit$variable.importance}))  %&gt;%
  tidyr::expand_grid(parameters) %&gt;% 
  dplyr::mutate(imp_rf = purrr::pmap(
    list(gamma_f, lambda_0_f, train, importances_std), type = &quot;rf&quot;, 
    penalization), 
    imp_mi = purrr::pmap(
      list(gamma_f, lambda_0_f, train, importances_std), type = &quot;MI&quot;, penalization)) </code></pre>
</div>
A quick look at :
<div class="layout-chunk" data-layout="l-body">
<pre><code>
# A tibble: 3 x 7
  id    reg_factor     mtry lambda_0_f gamma_f imp_rf      imp_mi     
  &lt;chr&gt; &lt;list&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;      &lt;list&gt;     
1 Fold1 &lt;dbl [2,905]&gt;   0.2       0.35     0.3 &lt;dbl [2,90… &lt;dbl [2,90…
2 Fold1 &lt;dbl [2,905]&gt;   0.2       0.35     0.5 &lt;dbl [2,90… &lt;dbl [2,90…
3 Fold1 &lt;dbl [2,905]&gt;   0.2       0.35     0.8 &lt;dbl [2,90… &lt;dbl [2,90…</code></pre>
</div>
<p>The <code>folds_imp</code> object has 90 rows, since it is the combination of 2 <span class="math inline">\(\times\)</span> 3 <span class="math inline">\(\times\)</span> 3 hyperparameter combinations for each of the 5 training sets, and 2 different <span class="math inline">\(g(\mathbf{x}_i)\)</span>. Before running our penalized models, we take a look at the results for the standard random forests models (the <code>model</code> column). Here, the <code>accuracy</code> and <code>accuracy_std</code> columns represent the test accuracy and training accuracy from a non-penalized RF, which was run before to create the penalization coefficients, so now we can use it for comparison:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
folds_imp %&gt;% 
  dplyr::group_by(id) %&gt;% 
  dplyr::slice(1) %&gt;% 
  dplyr::ungroup() %&gt;% 
  dplyr::select(id, model, train, test) %&gt;% 
  dplyr::mutate(
    model_importance = purrr::map(model, ~{.x$fit$variable.importance}),
    n_var = purrr::map_dbl(model_importance, n_vars), 
    accuracy_test_std = purrr::map2_dbl(
      .x = model, .y = test, ~{ acc_test(.x, test = .y)}),
    accuracy_std = 1 -purrr::map_dbl(model, ~{ .x$fit$prediction.error})
  ) %&gt;% 
  dplyr::select(id, n_var, accuracy_test_std, accuracy_std) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-condensed table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
id
</th>
<th style="text-align:right;">
n_var
</th>
<th style="text-align:right;">
accuracy_test_std
</th>
<th style="text-align:right;">
accuracy_std
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
1206
</td>
<td style="text-align:right;">
0.735
</td>
<td style="text-align:right;">
0.8481872
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold2
</td>
<td style="text-align:right;">
1380
</td>
<td style="text-align:right;">
0.765
</td>
<td style="text-align:right;">
0.8235503
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold3
</td>
<td style="text-align:right;">
1379
</td>
<td style="text-align:right;">
0.765
</td>
<td style="text-align:right;">
0.8246055
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold4
</td>
<td style="text-align:right;">
1377
</td>
<td style="text-align:right;">
0.788
</td>
<td style="text-align:right;">
0.8260160
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold5
</td>
<td style="text-align:right;">
1198
</td>
<td style="text-align:right;">
0.667
</td>
<td style="text-align:right;">
0.8548713
</td>
</tr>
</tbody>
</table>
</div>
<p>The following code runs all the penalized random forests models and calculates their metrics.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
run_all_models &lt;-  folds_imp %&gt;%   
  dplyr::select(id, model, train, test,  imp_rf, imp_mi, mtry, lambda_0_f, gamma_f) %&gt;% 
  tidyr::gather(type, importance, -train, -test, -mtry,-id, -model, -lambda_0_f, -gamma_f) %&gt;% 
  dplyr::mutate(fit_penalized_rf = purrr::pmap(list(train, importance, mtry), modelling)) </code></pre>
</div>
<p>And finally we extract the metrics we’re interested in, from each estimated model: the number of features used, accuracy in the test set, and the accuracy calculated during training:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
results &lt;- run_all_models %&gt;% 
  dplyr::mutate(
    model_importance = purrr::map(fit_penalized_rf, ~{.x$fit$variable.importance}),
    n_var = purrr::map_dbl(model_importance, n_vars),
    accuracy = 1 - purrr::map_dbl(fit_penalized_rf, ~{ .x$fit$prediction.error}),
    accuracy_test = purrr::map2_dbl(
      .x = fit_penalized_rf, .y = test, ~{ acc_test(.x, .y)})) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
A quick look at the <code>results</code> object:
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
results %&gt;% 
  dplyr::arrange(id, desc(accuracy_test), desc(accuracy), n_var) %&gt;% 
  dplyr::slice(1:5) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-condensed table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
id
</th>
<th style="text-align:right;">
mtry
</th>
<th style="text-align:right;">
lambda_0_f
</th>
<th style="text-align:right;">
gamma_f
</th>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
n_var
</th>
<th style="text-align:right;">
accuracy
</th>
<th style="text-align:right;">
accuracy_test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
0.85
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
imp_rf
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.8979667
</td>
<td style="text-align:right;">
0.824
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
imp_rf
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
0.8786310
</td>
<td style="text-align:right;">
0.794
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:left;">
imp_rf
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.9001406
</td>
<td style="text-align:right;">
0.765
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:left;">
imp_rf
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.8998857
</td>
<td style="text-align:right;">
0.765
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold1
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
imp_rf
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
0.8995989
</td>
<td style="text-align:right;">
0.765
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="post_files/figure-html5/unnamed-chunk-12-1.png" alt="Figure 1. Test accuracies for each combination of mtry, type of $g(\mathbf{x}_i)$, and $\gamma$." width="1344" />
<p class="caption">
Figure 1: Figure 1. Test accuracies for each combination of mtry, type of <span class="math inline">\(g(\mathbf{x}_i)\)</span>, and <span class="math inline">\(\gamma\)</span>.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="post_files/figure-html5/unnamed-chunk-13-1.png" alt="Figure 2. Final number of variables used for each combination of mtry, type of $g(\mathbf{x}_i)$, and $\gamma$" width="1344" />
<p class="caption">
Figure 2: Figure 2. Final number of variables used for each combination of mtry, type of <span class="math inline">\(g(\mathbf{x}_i)\)</span>, and <span class="math inline">\(\gamma\)</span>
</p>
</div>
</div>
<p>From the plots above, we can have an idea of what the test accuracies (Figure 1) and final number of variables used (Figure 2) is for each combination of <code>mtry</code> (in percentage of variables used), and type of <span class="math inline">\(g(\mathbf{x}_i)\)</span> (using a mutual information function or boosted by a standard RF), marginalized over <span class="math inline">\(\lambda_0\)</span>. Comparing that to the test accuracy (average of 0.744) and number of variables used (average of 1308) of the standard random forest, we can see that there has been a good improvement, since most models have a simelar accuracy to a full random forest, but are using many fewer features (from a maximum of around 30 to a minimum of around 5 features). Regarding the hyperparameter configurations, it seems that using the normalized importance values of a standard random forest as <span class="math inline">\(g(\mathbf{x}_i)\)</span> leads to the best test accuracy results overall, but with more variation across the different <code>mtry</code> values. As for the number of features used, using the normalized importance values of a standard random forest as <span class="math inline">\(g(\mathbf{x}_i)\)</span> results in using just a few variables, also with a bigger variation across <code>mtry</code> values. The number of features used for this scenario gets very low, which can be very attractive if we’re worried about using the least variables as possible.</p>
<p>Now, following what was described before as the ‘full feature selection procedure’, let’s move on to the next step: selecting the best penalized models for each training set and reevaluating them. In the next code chunks, we get the top-3 models for each training id, arranging first by test accuracy, training accuracy and number of variables. After that, we create the new model formulas for each model, rerun the random forest algorithm with each feature set, for each of the 5 training sets and evaluate their results:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
best_models &lt;- results %&gt;% 
  arrange(desc(accuracy_test), desc(accuracy), n_var) %&gt;% 
  group_by(id) %&gt;% 
  slice(1:3) %&gt;% 
  ungroup() %&gt;% 
  mutate(new_formula = map(model_importance, get_formula))

# Re-evaluating selected variables -----------------
reev &lt;- tibble(forms = best_models$new_formula) %&gt;% 
  tidyr::expand_grid(folds) %&gt;% 
  dplyr::mutate(reev_models = purrr::map2(train, forms, modelling_reev))

results_reev &lt;- reev %&gt;% 
  dplyr::mutate(feat_importance = purrr::map(reev_models, ~{.x$fit$variable.importance}),
                n_var = purrr::map_dbl(feat_importance, n_vars),
                accuracy = 1 - purrr::map_dbl(reev_models, ~{ .x$fit$prediction.error}),
                accuracy_test = purrr::map2_dbl(.x = reev_models, .y = test, ~{ acc_test(.x, test = .y)})) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-condensed table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
id
</th>
<th style="text-align:right;">
n_var
</th>
<th style="text-align:right;">
accuracy
</th>
<th style="text-align:right;">
accuracy_test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Fold2
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
0.8638687
</td>
<td style="text-align:right;">
0.971
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold2
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
0.8768484
</td>
<td style="text-align:right;">
0.941
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold2
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.8650016
</td>
<td style="text-align:right;">
0.941
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold3
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.8607979
</td>
<td style="text-align:right;">
0.941
</td>
</tr>
<tr>
<td style="text-align:left;">
Fold2
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.8566280
</td>
<td style="text-align:right;">
0.941
</td>
</tr>
</tbody>
</table>
</div>
<p>The accuracy values are looking very good now, even for the test set. Note that the <code>results_reev</code> object has 75 rows, since we have run 15 random forests models for each of the 5 training sets. We still need to reduce this number, so the last step of our methods consists of gathering the most used features by such models, and creating one final algorithm. This final model will be evaluated in 20 training and test sets, so we can be more certain about its accuracy results. In the following code, we select the top best 30 fitted models (in terms of the accuracies and number of features used) find the 15 features most used by them, and fit a random forest model in the 20 new training and test sets using this final 15-features set.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
selected_vars &lt;- results_reev %&gt;% 
  arrange(desc(accuracy_test), desc(accuracy), n_var) %&gt;% 
  slice(1:30) %&gt;% 
  mutate(ind = 1:n(), vars = map(feat_importance, get_vars)) %&gt;% 
  dplyr::select(ind, vars) %&gt;% 
  unnest() %&gt;% 
  group_by(vars) %&gt;% 
  summarise(count = n()) %&gt;% 
  arrange(desc(count))

# Select the final 15 features
final_vars &lt;- selected_vars %&gt;% slice(1:15) %&gt;% pull(vars)
# Create the final formula 
final_form &lt;- paste(&quot;class ~ &quot;, paste0(final_vars, collapse = &#39; + &#39;)) %&gt;%
  as.formula()

# Create the 20 new training and test sets
set.seed(2021)
folds_20 &lt;- rsample::vfold_cv(gravier, v = 20) %&gt;% 
  dplyr::mutate(train =  map(splits, training), test  = map(splits, testing))

# Run the final model for the new train-test sets
final_results &lt;- folds_20$splits %&gt;% map(~{
  train &lt;-  training(.x)
  test &lt;-  testing(.x)
  
  rf &lt;- rand_forest(trees = 500, mtry = 7) %&gt;%
    set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;% 
    set_mode(&quot;classification&quot;) %&gt;% 
    parsnip::fit(final_form, data = train)
  
  accuracy_test &lt;- acc_test(rf, test = test)
  list(accuracy_test = accuracy_test, 
       accuracy = 1 - rf$fit$prediction.error, 
       imp = rf$fit$variable.importance)
})</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>The accuracy averages and medians for this final model are shown below. We can see that the final test accuracy (average) is higher than what was seen in the previous plots, but now using only 15 features. At last, we show the variable importance plot for the 15 features used, arranged by importance order. This plot informs us about which variables helped the predictions the most, and we can see that the most important feature really dominates the plot.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
data.frame(accuracy_test = final_results %&gt;% map_dbl(&quot;accuracy_test&quot;), 
           accuracy = final_results %&gt;% map_dbl(&quot;accuracy&quot;)) %&gt;% 
  gather(type, value) %&gt;% 
  group_by(type) %&gt;% 
  summarise(mean = mean(value), median = median(value)) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<table class="table table-condensed table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
median
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
accuracy
</td>
<td style="text-align:right;">
0.8974934
</td>
<td style="text-align:right;">
0.8971951
</td>
</tr>
<tr>
<td style="text-align:left;">
accuracy_test
</td>
<td style="text-align:right;">
0.8681000
</td>
<td style="text-align:right;">
0.8820000
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="post_files/figure-html5/unnamed-chunk-21-1.png" alt="Figure 3. Average importance values for the final selected variables" width="480" />
<p class="caption">
Figure 3: Figure 3. Average importance values for the final selected variables
</p>
</div>
</div>
<h3 id="how-does-this-compare-to-the-literature">How does this compare to the literature?</h3>
<p>This post only intends to quickly demonstrate how the feature selection via gain penalization can be used, but we can also compare our results to a few others that have come up in similar literature that used the same dataset:</p>
<ul>
<li><span class="citation" data-cites="huynh2020improvements">(Huynh, Do, and others <a href="#ref-huynh2020improvements" role="doc-biblioref">2020</a>)</span> reports a maximum accuracy of 84.52% (page 10)</li>
<li><span class="citation" data-cites="lopez2018double">(López, Maldonado, and Carrasco <a href="#ref-lopez2018double" role="doc-biblioref">2018</a>)</span> reports a maximum AUC of 79.7 (page 384)</li>
<li><span class="citation" data-cites="takada2018independently">(Takada, Suzuki, and Fujisawa <a href="#ref-takada2018independently" role="doc-biblioref">2018</a>)</span> reports a maximum miscalssification accuracy of ~75% (page 9)</li>
</ul>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-Breiman1996">
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2): 123–40. <a href="https://doi.org/10.1023/A:1018054314350">https://doi.org/10.1023/A:1018054314350</a>.</p>
</div>
<div id="ref-rrf_paper">
<p>Deng, Houtao, and George Runger. 2012. “Feature Selection via Regularized Trees.” In <em>The 2012 International Joint Conference on Neural Networks (Ijcnn)</em>, 1–8. IEEE.</p>
</div>
<div id="ref-gravier2010prognostic">
<p>Gravier, Eléonore, Gaëlle Pierron, Anne Vincent-Salomon, Nadège Gruel, Virginie Raynal, Alexia Savignoni, Yann De Rycke, et al. 2010. “A Prognostic Dna Signature for T1t2 Node-Negative Breast Cancer Patients.” <em>Genes, Chromosomes and Cancer</em> 49 (12): 1125–34.</p>
</div>
<div id="ref-huynh2020improvements">
<p>Huynh, Phuoc-Hai, Thanh-Nghi Do, and others. 2020. “Improvements in the Large P, Small N Classification Issue.” <em>SN Computer Science</em> 1 (4): 1–19.</p>
</div>
<div id="ref-phdthesisRF">
<p>Louppe, Gilles. 2014. “Understanding Random Forests: From Theory to Practice.” PhD thesis. <a href="https://doi.org/10.13140/2.1.1570.5928">https://doi.org/10.13140/2.1.1570.5928</a>.</p>
</div>
<div id="ref-lopez2018double">
<p>López, Julio, Sebastián Maldonado, and Miguel Carrasco. 2018. “Double Regularization Methods for Robust Feature Selection and Svm Classification via Dc Programming.” <em>Information Sciences</em> 429: 377–89.</p>
</div>
<div id="ref-takada2018independently">
<p>Takada, Masaaki, Taiji Suzuki, and Hironori Fujisawa. 2018. “Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables.” In <em>International Conference on Artificial Intelligence and Statistics</em>, 454–63. PMLR.</p>
</div>
<div id="ref-wundervald2020generalizing">
<p>Wundervald, Bruna, Andrew C Parnell, and Katarina Domijan. 2020. “Generalizing Gain Penalization for Feature Selection in Tree-Based Models.” <em>IEEE Access</em> 8: 190231–9.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="citation">Citation</h3>
<p>For attribution, please cite this work as</p>
<pre class="citation-appendix short">Wundervald, "Feature Selection via Gain Penalization in Random Forests", IEEE Access, 2021</pre>
<p>BibTeX citation</p>
<pre class="citation-appendix long">@article{wundervald2021feature,
  author = {Wundervald, Bruna},
  title = {Feature Selection via Gain Penalization in Random Forests},
  journal = {IEEE Access},
  year = {2021},
  note = {https://ieeexplore.ieee.org/document/9229097},
  doi = {10.1109/ACCESS.2020.3032095}
}</pre>
</div>
<script id="distill-bibliography" type="text/bibtex">
@Article{Breiman1996,
  author="Breiman, Leo",
  title="Bagging Predictors",
  journal="Machine Learning",
  year="1996",
  month="Aug",
  day="01",
  volume="24",
  number="2",
  pages="123--140",
  issn="1573-0565",
  doi="10.1023/A:1018054314350",
  url="https://doi.org/10.1023/A:1018054314350"
}

@article{lopez2018double,
  title={Double regularization methods for robust feature selection and SVM classification via DC programming},
  author={L{\'o}pez, Julio and Maldonado, Sebasti{\'a}n and Carrasco, Miguel},
  journal={Information Sciences},
  volume={429},
  pages={377--389},
  year={2018},
  publisher={Elsevier}
}

@article{gravier2010prognostic,
  title={A prognostic DNA signature for T1T2 node-negative breast cancer patients},
  author={Gravier, El{\'e}onore and Pierron, Ga{\"e}lle and Vincent-Salomon, Anne and Gruel, Nad{\`e}ge and Raynal, Virginie and Savignoni, Alexia and De Rycke, Yann and Pierga, Jean-Yves and Lucchesi, Carlo and Reyal, Fabien and others},
  journal={Genes, chromosomes and cancer},
  volume={49},
  number={12},
  pages={1125--1134},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{takada2018independently,
  title={Independently interpretable lasso: A new regularizer for sparse regression with uncorrelated variables},
  author={Takada, Masaaki and Suzuki, Taiji and Fujisawa, Hironori},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={454--463},
  year={2018},
  organization={PMLR}
}


@article{huynh2020improvements,
  title={Improvements in the Large p, Small n Classification Issue},
  author={Huynh, Phuoc-Hai and Do, Thanh-Nghi and others},
  journal={SN Computer Science},
  volume={1},
  number={4},
  pages={1--19},
  year={2020},
  publisher={Springer}
}

@phdthesis{phdthesisRF,
  author = {Louppe, Gilles},
  year = {2014},
  month = {10},
  pages = {},
  title = {Understanding Random Forests: From Theory to Practice},
  doi = {10.13140/2.1.1570.5928}
}

@article{wundervald2020generalizing,
  title={Generalizing Gain Penalization for Feature Selection in Tree-Based Models},
  author={Wundervald, Bruna and Parnell, Andrew C and Domijan, Katarina},
  journal={IEEE Access},
  volume={8},
  pages={190231--190239},
  year={2020},
  publisher={IEEE}
}

@inproceedings{rrf_paper,
  title={Feature selection via regularized trees},
  author={Deng, Houtao and Runger, George},
  booktitle={The 2012 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2012},
  organization={IEEE}
}

</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
